{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy e textblob\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: ***@sabrina49834572***\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'RedBull'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t2])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t2:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.[OK]<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.[OK]\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readerTrain = pd.read_excel('{0}.xlsx'.format(produto))\n",
    "readerTest = pd.read_excel('{0}.xlsx'.format(produto),sheetname = 'Teste')\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in range(0,t):\n",
    "    train.append([readerTrain['Treinamento'][i], readerTrain['Util'][i]])\n",
    "\n",
    "for i in range(0,n-t):\n",
    "    test.append([readerTest['Teste'][i],readerTest['Util'][i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multiply(n):\n",
    "    total = 1\n",
    "    for i in range(0, len(n)):\n",
    "        total *= n[i]\n",
    "    print(total)\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.YesFreq = 0\n",
    "        self.NoFreq = 0\n",
    "        self.WordFreqList = [[\"RedBull\",0,0]]\n",
    "                            #[Palavra, FreqNo, FreqYes]\n",
    "        self.TotalWord = [1,1,1]\n",
    "                            #Palavra, FreqNo, FreqYes\n",
    "        \n",
    "    def train(self,train):\n",
    "        for tweet in train:\n",
    "            if tweet[1]: self.YesFreq+=1 #1 = Coluna Util\n",
    "            else: self.NoFreq+=1\n",
    "            tweet[0] = self.normalizeTweet(tweet[0])#0 = Coluna Treinamento\n",
    "            WordList = tweet[0].split(' ') #0 = Coluna Treinamento\n",
    "            for word in WordList:\n",
    "                onList = False\n",
    "                for freq in self.WordFreqList:\n",
    "                    if word == freq[0]:\n",
    "                        onList = True\n",
    "                        if tweet[1]: freq[2]+=1 #1 = Coluna Util\n",
    "                        else: freq[1]+=1\n",
    "                \n",
    "                if not onList:\n",
    "                    if tweet[1]: self.WordFreqList.append([word,0,1]) #1 - Coluna Util\n",
    "                    else: self.WordFreqList.append([word,1,0])\n",
    "        return 0\n",
    "    \n",
    "    def analyze(self,tweet):\n",
    "        tweet[0] = self.normalizeTweet(tweet[0])#0 = Coluna Treinamento\n",
    "        WordList = tweet[0].split(' ') #0 = Coluna Treinamento\n",
    "        relevancia = []\n",
    "        probWord = []\n",
    "        for word in WordList:\n",
    "            onList = False\n",
    "            for freq in self.WordFreqList:\n",
    "                if word == freq[0]:\n",
    "                    onList = True\n",
    "                    probNo = (freq[1]+1)/self.TotalWord[1]\n",
    "                    probYes = (freq[2]+1)/self.TotalWord[2]\n",
    "                    probTotal = (freq[1]+freq[2]+1)/self.TotalWord[0]\n",
    "                    probWord.append([probTotal,probNo,probYes])\n",
    "                    \n",
    "            if not onList:\n",
    "                probWord.append([1/self.TotalWord[0],1/self.TotalWord[1],1/self.TotalWord[2]])\n",
    "        \n",
    "        probTweet = multiply(probWord[:][1])\n",
    "        print(probTweet)\n",
    "    \n",
    "    def normalizeTweet(self,tweet):\n",
    "        tweet.lower()\n",
    "        tweet = tweet.replace(',',' ')\n",
    "        tweet = tweet.replace('!',' ')\n",
    "        tweet = tweet.replace('$',' ')\n",
    "        tweet = tweet.replace('%',' ')\n",
    "        tweet = tweet.replace('&',' ')\n",
    "        tweet = tweet.replace('*',' ')\n",
    "        tweet = tweet.replace('(',' ')\n",
    "        tweet = tweet.replace(')',' ')\n",
    "        tweet = tweet.replace('-',' ')\n",
    "        tweet = tweet.replace('+',' ')\n",
    "        tweet = tweet.replace('=',' ')\n",
    "        tweet = tweet.replace('_',' ')\n",
    "        tweet = tweet.replace('.',' ')\n",
    "        tweet = tweet.replace(':',' ')\n",
    "        tweet = tweet.replace(';',' ')\n",
    "        tweet = tweet.replace('?',' ')\n",
    "        tweet = tweet.replace('/',' ')\n",
    "        tweet = tweet.replace('\\n',' ')\n",
    "        for i in range(0,4): tweet = tweet.replace('  ',' ')\n",
    "        return tweet\n",
    "                              \n",
    "    def show(self):\n",
    "        print(self.WordFreqList)\n",
    "        print()\n",
    "        print(\"SIMs: \" + str(self.YesFreq))\n",
    "        print(\"NÃOs: \" + str(self.NoFreq))\n",
    "        print(\"TOTAL: \" + str(self.YesFreq+self.NoFreq))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "None\n",
      "[['RedBull', 0, 0], ['rt', 212, 0], ['@ieuwesley', 30, 0], ['sabe', 137, 2], ['aquele', 134, 0], ['gelo', 138, 0], ['que', 264, 3], ['você', 139, 0], ['me', 144, 0], ['deu', 135, 0], ['eu', 168, 4], ['tô', 134, 0], ['tomando', 136, 2], ['ele', 135, 0], ['na', 146, 0], ['balada', 134, 0], ['com', 151, 1], ['whisky', 146, 0], ['e', 195, 5], ['redbull', 222, 13], ['@manodieguinhoo', 102, 0], ['@akuitagawa', 1, 1], ['da', 63, 0], ['ultima', 1, 0], ['vez', 1, 1], ['nao', 5, 1], ['foi', 4, 0], ['1garrafa', 1, 0], ['monster', 1, 0], ['verdinha', 1, 0], ['aquela', 2, 0], ['resenha', 1, 0], ['antes', 1, 0], ['do', 48, 1], ['futebol', 1, 0], ['⚽️', 1, 0], ['#resenha', 1, 0], ['#futebol', 1, 0], ['#hosoccerbrasil', 1, 0], ['#nike', 1, 0], ['#adidas', 1, 0], ['#redbull…', 1, 0], ['https', 41, 2], ['t', 38, 2], ['co', 38, 2], ['jjqzvbcoar', 1, 0], ['@solteirandoo', 1, 0], ['quinta', 1, 0], ['feira', 1, 0], ['é', 115, 2], ['feriado', 2, 0], ['iyuaxn6zfu', 1, 0], ['novo', 3, 0], ['status', 1, 0], ['pessoas', 1, 0], ['visitaram', 1, 0], ['meu', 6, 0], ['perfil', 1, 0], ['rcjqkyu1sx', 1, 0], ['@saarcristina', 0, 1], ['nós', 0, 1], ['vamos', 2, 1], ['amiga', 0, 1], ['pq', 2, 2], ['esqueci', 0, 1], ['de', 95, 10], ['falar', 0, 1], ['pequeno', 0, 1], ['detalhe', 0, 1], ['open', 1, 1], ['bar', 1, 1], ['absolut', 28, 1], ['😂🙌🏼', 0, 1], ['onde', 1, 0], ['q', 28, 1], ['tá', 8, 2], ['a', 42, 2], ['umas', 2, 0], ['ganzas', 1, 0], ['qnd', 2, 0], ['um', 16, 4], ['gajo', 1, 0], ['em', 3, 1], ['baixo', 1, 0], ['ah', 1, 0], ['pois', 1, 0], ['n', 74, 2], ['tão', 1, 0], ['fiquei', 1, 0], ['sem', 24, 0], ['polen', 1, 0], ['', 33, 4], ['@pereira', 2, 0], ['vivii', 2, 0], ['reclama', 27, 0], ['energético', 27, 0], ['não', 37, 2], ['vodka', 28, 0], ['mas', 27, 3], ['engole', 27, 0], ['leite', 27, 0], ['vaca', 25, 0], ['vai', 7, 1], ['entender', 4, 0], ['uma', 12, 1], ['gozada', 1, 0], ['pediu', 1, 0], ['pra', 27, 1], ['descansar', 1, 0], ['toma', 2, 0], ['prostituta', 1, 0], ['quer', 2, 0], ['gozar', 1, 0], ['bate', 3, 0], ['punheta', 1, 0], ['quero', 3, 1], ['seu', 1, 0], ['pau', 1, 0], ['pé', 1, 0], ['debora', 2, 0], ['\"', 3, 0], ['está', 2, 0], ['relacionamento', 1, 0], ['sério\"', 1, 0], ['guria', 1, 0], ['lá', 2, 0], ['olha', 2, 0], ['rayana', 1, 0], ['aaaaaaa', 1, 0], ['vi', 1, 0], ['kkkkkkkkkkkkkkkkkk', 1, 0], ['prefiro', 1, 0], ['tuu', 1, 0], ['😌', 1, 0], ['@bravonline', 3, 0], ['ouviram', 4, 0], ['céu', 6, 0], ['boogarins', 5, 0], ['pioprzhvld', 4, 0], ['@tremdogd', 1, 0], ['hoje', 24, 0], ['comprei', 4, 0], ['adidas', 1, 0], ['no', 24, 1], ['lugar', 1, 0], ['puma', 1, 0], ['disk', 1, 0], ['água', 9, 0], ['wisk', 3, 0], ['🎵', 1, 0], ['#mcmaxnarda', 1, 0], ['@marcos', 1, 0], ['eobichoo', 1, 0], ['paciência', 1, 0], ['forte', 1, 1], ['@screamyell', 1, 0], ['assista', 2, 0], ['ao', 3, 0], ['papo', 2, 0], ['reuniu', 2, 0], ['@simsaopaulo', 2, 0], ['@naturamusical', 2, 0], ['station', 2, 0], ['@mmrecords', 2, 0], ['lariu', 2, 0], ['@lucas', 2, 0], ['santtana', 1, 0], ['t…', 1, 0], ['@zustavo', 21, 0], ['oii', 21, 0], ['meninas', 21, 0], ['tutorial', 21, 0], ['vou', 22, 0], ['ensinar', 21, 0], ['como', 23, 0], ['levantar', 21, 0], ['escova', 21, 0], ['os', 26, 0], ['dente', 21, 0], ['perder', 21, 0], ['o', 42, 2], ['sono', 21, 0], ['parecer', 21, 0], ['vc', 23, 0], ['tomou', 21, 0], ['3', 21, 0], ['li…', 21, 0], ['@marcmarquez93', 1, 0], ['@oakley', 1, 0], ['@redbull', 3, 0], ['@motogp', 1, 0], ['boa', 4, 0], ['sorte', 1, 0], ['rumo', 1, 0], ['aos', 1, 0], ['4', 1, 1], ['títulos', 1, 0], ['mundiais', 1, 0], ['mal', 2, 0], ['3bcomtyb37', 1, 0], ['@kaiobarbosa14', 3, 0], ['embrazadão', 8, 0], ['hu', 16, 0], ['reboladinha', 4, 0], ['bulu', 4, 0], ['🎶', 8, 0], ['@lcgila07', 3, 0], ['ia', 3, 0], ['cair', 3, 0], ['bem', 6, 0], ['agora', 3, 0], ['uns', 3, 0], ['copões', 3, 0], ['c', 3, 0], ['coco', 3, 0], ['gelado', 3, 0], ['chega', 8, 0], ['dá', 4, 0], ['boca', 3, 0], ['😍', 3, 0], ['pessoalmente', 0, 1], ['to', 2, 1], ['achando', 1, 1], ['muito', 0, 1], ['mais', 2, 3], ['tomei', 1, 1], ['duas', 1, 1], ['latinhas', 0, 1], ['menos', 0, 1], ['1h', 0, 1], ['to…', 0, 1], ['0syd7ikhmh', 0, 1], ['nos', 4, 1], ['baseado', 2, 1], ['fumando', 2, 1], ['@canalmasterking', 1, 0], ['@youtube', 2, 0], ['coloca', 1, 0], ['lata', 1, 0], ['capiroto', 1, 0], ['cade', 1, 0], ['minha', 3, 0], ['melhor', 3, 0], ['amigaa', 1, 0], ['@ferrson', 16, 0], ['cerveja', 24, 0], ['stella', 24, 0], ['@inesrebelop', 2, 0], ['porque', 3, 0], ['aviões', 2, 0], ['conseguiram', 2, 0], ['atrasar', 2, 0], ['todos', 2, 0], ['comboios', 2, 0], ['tipo', 2, 0], ['escolham', 2, 0], ['meio', 3, 0], ['locomoção', 2, 0], ['atrapalhar…', 2, 0], ['químicas', 1, 0], ['loucuras', 1, 0], ['erva', 1, 0], ['venenosa', 1, 0], ['deixou', 2, 0], ['susu', 1, 0], ['@relatoufrase', 1, 0], ['te', 7, 0], ['amo', 1, 0], ['prova', 1, 0], ['fubidetutc', 1, 0], ['casual', 0, 1], ['aquie', 0, 1], ['fingindo', 0, 1], ['nunca', 2, 2], ['tive', 0, 2], ['crise', 0, 1], ['ansiedade', 0, 1], ['após', 0, 1], ['consumir', 0, 1], ['esse', 0, 1], ['porduotinho', 0, 1], ['fiz', 0, 1], ['essa', 3, 1], ['escolha', 0, 1], ['deus', 1, 1], ['@sandro3sss', 1, 0], ['sair', 1, 1], ['hj', 2, 0], ['dar', 1, 0], ['aniversário', 2, 0], ['pug', 1, 0], ['tinha', 2, 0], ['outra', 1, 0], ['desculpa', 1, 0], ['#tercadetremurasdv', 1, 0], ['…', 2, 0], ['@luna', 1, 0], ['quint', 1, 0], ['perdeu', 2, 0], ['eles', 2, 0], ['bebendo', 2, 0], ['thalita', 2, 0], ['rebouças', 2, 0], ['ltwtb2tlr9', 2, 0], ['@afrontamesmobb', 1, 0], ['primeiro', 2, 0], ['surto', 1, 0], ['depois', 1, 0], ['converso', 1, 0], ['@larissa', 1, 0], ['kamili', 1, 0], ['que…', 1, 0], ['@faleicria', 1, 0], ['tenho', 1, 0], ['fzhjg2tcxk', 1, 0], ['oi', 1, 0], ['quem', 2, 0], ['nova', 1, 0], ['estagiaria', 1, 0], ['sbm', 1, 0], ['💁🏻', 1, 0], ['¿bnet', 1, 0], ['nacional', 1, 0], ['2018', 1, 0], ['gallos', 1, 0], ['@planejaram', 1, 0], ['tudo', 1, 0], ['posso', 1, 1], ['fortalece', 1, 0], ['@matheusmmoreno', 1, 0], ['4x4', 2, 0], ['tempo', 2, 0], ['voa', 2, 0], ['quanta', 2, 0], ['mulher', 2, 0], ['uauuuu', 1, 0], ['ahazo', 1, 0], ['—', 5, 0], ['fato', 2, 0], ['isso', 1, 0], ['pe8dy1tecj', 1, 0], ['cheguei', 1, 0], ['fase', 1, 0], ['ponto', 1, 0], ['chegamos', 1, 0], ['@bilyguerreiro', 1, 0], ['fará', 1, 0], ['voar', 1, 0], ['🦋', 1, 0], ['@jkduartt', 2, 0], ['cara', 1, 0], ['panos', 1, 0], ['chão', 1, 0], ['aqui', 2, 0], ['casa', 1, 0], ['criar', 1, 0], ['asa', 1, 0], ['só', 6, 0], ['pode', 2, 0], ['jaco', 1, 0], ['pro', 2, 0], ['lucca', 1, 0], ['chorando', 1, 0], ['pagar', 1, 0], ['@boogarins', 1, 0], ['domingão', 1, 0], ['largadão', 1, 0], ['coisa', 1, 0], ['maravilhosa', 1, 0], ['entre', 1, 0], ['idas', 1, 0], ['vindas', 1, 0], ['conseguimos', 1, 0], ['reunir', 1, 0], ['estudios', 1, 0], ['vinho', 2, 0], ['gasosa', 1, 0], ['forneçam', 1, 0], ['tal', 1, 0], ['🙏🏽😅😅😅', 1, 0], ['advinhem', 1, 0], ['ganhei', 1, 0], ['professor', 1, 0], ['matemática', 1, 0], ['amava', 1, 0], ['até', 1, 0], ['abrir', 2, 0], ['teu', 2, 0], ['cc', 1, 0], ['tt', 1, 0], ['😔', 1, 0], ['kkkkkkkkkkkkkkk', 1, 0], ['para', 1, 0], ['drama', 1, 0], ['débora', 2, 0], ['7k1fpy6jxf', 1, 0], ['alguém', 1, 1], ['aonde', 0, 1], ['tem', 5, 1], ['zero', 0, 1], ['vender', 0, 1], ['fui', 0, 1], ['num', 0, 1], ['posto', 1, 1], ['gasolina', 0, 2], ['consigo', 0, 1], ['decidir', 0, 1], ['maior', 0, 1], ['roubo', 0, 1], ['1l', 0, 1], ['169', 0, 1], ['ou', 0, 1], ['237ml', 0, 1], ['21', 0, 1], ['90', 0, 1], ['santtana…', 1, 0], ['dizuc4up3m', 1, 0], ['@dunygith', 1, 0], ['quando', 2, 0], ['as', 2, 0], ['inimigas', 1, 0], ['ficam', 1, 0], ['olhando', 1, 0], ['rindo', 1, 0], ['3gt4y9jm93', 1, 0], ['@feleepoynter', 1, 0], ['neymar', 2, 0], ['batendo', 2, 0], ['escanteio', 2, 0], ['igual', 2, 0], ['pedro', 2, 0], ['nem', 2, 0], ['pequena', 2, 0], ['área', 2, 0], ['bandido', 1, 0], ['uísque', 1, 0], ['sdd', 1, 0], ['desse', 1, 0], ['rolê', 2, 0], ['arroba', 1, 0], ['patrocina', 1, 0], ['gu76ovnanq', 1, 0], ['ambição', 1, 0], ['azas', 1, 0], ['tênis', 1, 0], ['nike', 2, 0], ['fuzil', 2, 0], ['@redbullpor', 2, 0], ['#redbullromaniacs', 1, 0], ['8', 2, 0], ['dicas', 1, 0], ['sobrevivência', 1, 0], ['@alfredogomez89', 1, 0], ['apt15m7c4p', 1, 0], ['#hardenduro', 1, 0], ['gepa9ao0yj', 1, 0], ['@untitledtt', 0, 1], ['ajjahahaha', 0, 1], ['proxima', 0, 1], ['tou', 0, 1], ['algarve', 0, 1], ['ainda', 1, 2], ['chance', 0, 1], ['@idiotalizaando', 1, 0], ['remédio', 1, 0], ['😆', 1, 0], ['mntzhyvlla', 1, 0], ['também', 1, 0], ['eh', 1, 0], ['agua', 1, 0], ['[#video]', 1, 0], ['#f1', 1, 0], ['2017', 2, 0], ['correndo', 1, 0], ['#redbull', 3, 0], ['monza', 1, 0], ['gameplay', 1, 0], ['pt', 1, 0], ['br', 1, 0], ['logitech', 1, 0], ['g27', 1, 0], ['fj3mz64wzv', 1, 0], ['acho', 1, 0], ['comprar', 1, 0], ['pack', 1, 0], ['vêm', 1, 0], ['aí', 1, 0], ['muitas', 1, 0], ['noitadas', 1, 0], ['@pfvrchato', 1, 0], ['chegam', 1, 0], ['mt', 1, 0], ['perto', 1, 0], ['crush', 2, 0], ['hkfi1nse0e', 1, 0], ['único', 1, 0], ['show', 1, 0], ['fim', 2, 0], ['semana', 2, 0], ['ligo', 1, 0], ['conecevwok', 1, 0], ['teve', 1, 0], ['tanto', 2, 0], ['medo', 1, 0], ['tanta', 1, 0], ['curiosidade', 1, 0], ['cruzar', 1, 0], ['dessas', 2, 0], ['confira', 1, 0], ['dqqfxtg7dz', 1, 0], ['@joanafsampaio', 1, 0], ['sou', 1, 0], ['única', 1, 0], ['porto', 2, 0], ['ver', 1, 0], ['cena', 2, 0], ['ficasse', 1, 0], ['sabendo', 1, 0], ['pamela', 1, 0], ['gosta', 1, 0], ['juliano', 1, 0], ['ba', 1, 0], ['pena', 1, 0], ['né', 1, 0], ['ela', 2, 0], ['gostar', 1, 0], ['dele', 1, 0], ['amar', 1, 0], ['😂❤', 2, 0], ['zrwptybau0', 1, 0], ['papai', 1, 0], ['receba', 1, 0], ['braços', 1, 0], ['abertos', 1, 0], ['#luto', 1, 0], ['neizin', 1, 0], ['redbull😖💜', 1, 0], ['fwq3x7vwwk', 1, 0], ['sabor', 1, 0], ['açai', 1, 0], ['novidade', 1, 0], ['verão', 1, 0], ['gkmzhhsr4w', 1, 0], ['@juancarmo00', 1, 0], ['traje', 1, 0], ['caixa', 1, 0], ['😅🙌🏼', 1, 0], ['@namoroupics', 1, 0], ['\"namorar', 1, 0], ['estraga', 1, 0], ['n5mnobefkq', 1, 0], ['más', 1, 0], ['😂😂😂', 1, 0], ['camisas', 1, 0], ['oakley', 1, 0], ['😏', 1, 0], ['@canalbotecof1', 0, 1], ['@porscheraces', 0, 1], ['ano', 0, 1], ['vem', 2, 1], ['tomo', 1, 0], ['maquina', 1, 0], ['metro', 1, 0], ['explicar', 1, 0], ['qual', 1, 1], ['air', 2, 0], ['race', 2, 0], ['@danielmelo66', 1, 0], ['demais', 1, 0], ['uaai', 1, 0], ['hahah', 1, 0], ['🤷🏼\\u200d♀️', 1, 0], ['\"você', 1, 0], ['significado', 1, 0], ['amor', 2, 0], ['próprio', 1, 0], ['mão', 1, 0], ['pessoa', 1, 0], ['ama', 1, 0], ['pelo', 1, 0], ['dela', 1, 0], ['fazer', 1, 0], ['@sugasfree', 1, 0], ['já', 1, 0], ['falei', 1, 0], ['faz', 1, 0], ['sua', 1, 0], ['teimosa', 1, 0], ['@neozinho', 1, 0], ['kkkkkkkkkkkkkkkkk', 1, 0], ['parece', 2, 0], ['jantou', 1, 0], ['lágrima', 1, 0], ['cai', 1, 0], ['aq', 1, 0], ['ckyglg1do2', 1, 0], ['saida', 1, 0], ['alcidao', 1, 0], ['kkkkkkkkkkkkk', 1, 0], ['tjvs4jksqx', 1, 0], ['@iza', 1, 0], ['bellals', 1, 0], ['va…', 1, 0], ['#airrace', 1, 0], ['histórica', 1, 0], ['cidade', 1, 0], ['🇵🇹', 1, 0], ['️é', 1, 0], ['dos', 3, 0], ['locais', 1, 0], ['preferidos', 1, 0], ['pilotos', 1, 0], ['fãs', 1, 0], ['vê', 1, 0], ['porquê', 1, 0], ['v7…', 1, 0], ['@tiomateus', 1, 0], ['dog', 1, 0], ['salva', 1, 0], ['foto', 1, 0], ['resto', 1, 0], ['espécie', 1, 0], ['usa', 1, 0], ['polo', 1, 0], ['pede', 1, 0], ['combo', 3, 0], ['red', 1, 0], ['label', 1, 0], ['@grleire', 1, 0], ['@salomon', 1, 0], ['spain', 1, 0], ['@salomonrunning', 1, 0], ['@suunto1esp', 1, 0], ['@suunto', 1, 0], ['grande', 1, 0], ['leire', 1, 0], ['bolt', 1, 0], ['bebe', 1, 0], ['amenizar', 1, 0], ['diria', 1, 0], ['lauro', 1, 0], ['jogaaaa', 1, 0], ['kkk', 2, 0], ['😂🌊🌊', 1, 0], ['asas', 1, 0], ['🤔', 1, 0], ['x96ypzyfnz', 1, 0], ['@iloveputarias', 1, 0], ['dias', 1, 0], ['nóis', 1, 0], ['cafuné', 1, 0], ['adianta', 1, 0], ['tomar', 1, 1], ['dei', 1, 0], ['se', 1, 1], ['3°', 1, 0], ['dose', 3, 0], ['fica', 1, 0], ['ligando', 1, 0], ['@preto', 2, 0], ['ze', 2, 0], ['amanhã', 2, 0], ['dupla', 2, 0], ['budweiser', 2, 0], ['skolbeats', 2, 0], ['orloff', 4, 0], ['ganha', 2, 0], ['garrafa', 2, 0], ['😱😱😱😱', 2, 0], ['htt…', 2, 0], ['probabilidade', 0, 1], ['morrer', 0, 1], ['ir', 0, 1], ['dormir', 0, 1], ['@redbullbr', 1, 0], ['comemorar', 1, 0], ['freddie', 1, 0], ['mercury', 1, 0], ['gente', 1, 0], ['cck5qq1d77', 1, 0], ['#freddie71', 1, 0], ['@queenwillrock', 1, 0], ['c…', 1, 0], ['rejeite', 0, 1], ['hvpuwpk2dt', 0, 1], ['gostei', 1, 0], ['vídeo', 1, 0], ['82arrjzywi', 1, 0], ['oporto', 1, 0], ['vontade', 0, 1], ['daquele', 0, 1], ['vermelho', 0, 1], ['parar', 0, 1], ['beber', 0, 2], ['sério', 0, 1], ['msm', 0, 1], ['@emancarvalho', 1, 0], ['ahaha', 1, 0], ['devo', 1, 0], ['levar', 1, 0], ['verde', 1, 0], ['seven', 1, 0], ['up', 1, 0], ['2', 1, 0], ['garrafões', 1, 0], ['quase', 1, 0], ['derrubei', 1, 0], ['enorme', 1, 0], ['frigobar', 1, 0], ['cv', 1, 0], ['tonta', 1, 0], ['rapaz', 1, 0], ['@dieguiiin', 1, 0], ['@ttavaresju', 1, 0], ['comigo', 1, 0], ['diego', 1, 0], ['#meubaileéfunk', 1, 0], ['ate', 1, 0], ['meia', 1, 0], ['noite', 1, 0], ['nome', 1, 0], ['lista', 1, 0], ['10', 2, 0], ['vida', 1, 0], ['amorrrr', 1, 0], ['❤', 1, 0], ['zrnrqzo1ki', 1, 0], ['pulserinha', 1, 0], ['raniel', 1, 0], ['pedido', 1, 0], ['@twdcitou', 1, 0], ['sofri', 1, 0], ['603kmimsmn', 1, 0], ['@vem', 0, 1], ['pokexota', 0, 1], ['nada', 0, 1], ['fardinho', 0, 1], ['ajude', 0, 1], ['disse', 1, 0], ['era', 1, 0], ['bom', 1, 0], ['favela', 1, 0], ['ouviu', 1, 0], ['nois', 1, 0], ['tb', 1, 0], ['tenis', 1, 0], ['@vieirajonathass', 1, 0], ['milagre', 1, 0], ['\"bebendo\"', 1, 0], ['rs', 1, 0], ['😂😂😂😂', 1, 0], ['latas', 1, 0], ['pqp', 1, 0], ['#rtp', 1, 0], ['#cliffdiving', 1, 0], ['transmissão', 1, 0], ['acontece', 1, 0], ['#fazadiferença', 1, 0], ['xnu5hzun6f', 1, 0], ['kkkkkkkkkkk', 1, 0]]\n",
      "\n",
      "SIMs: 13\n",
      "NÃOs: 287\n",
      "TOTAL: 300\n"
     ]
    }
   ],
   "source": [
    "Classificador = NaiveBayes()\n",
    "Classificador.train(train)\n",
    "Classificador.analyze(test[0])\n",
    "Classificador.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante.[  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivo Falso: 0.0%\n",
      "Positivo Verdadeiro: 0.0%\n",
      "Negativo Falso: 0.0%\n",
      "Negativo Verdadeiro: 0.0%\n",
      "\n",
      "Acerto: 0.0%\n",
      "Erro: 0.0%\n",
      "\n",
      "Total: 0.0%\n"
     ]
    }
   ],
   "source": [
    "fakePos = 0\n",
    "truePos = 0\n",
    "fakeNeg = 0\n",
    "trueNeg = 0\n",
    "\"\"\"\n",
    "for i in range(0,n-t):\n",
    "    if (cl.classify(test[i][0]) == test[i][1] and test[i][1] == 0): trueNeg+=1\n",
    "    elif (cl.classify(test[i][0]) == test[i][1] and test[i][1] == 1): truePos+=1\n",
    "    elif (cl.classify(test[i][0]) != test[i][1] and test[i][1] == 0): fakeNeg+=1\n",
    "    elif (cl.classify(test[i][0]) != test[i][1] and test[i][1] == 1): fakePos+=1\n",
    "\"\"\"\n",
    "print(\"Positivo Falso: \" + str((fakePos/(n-t))*100)+\"%\")\n",
    "print(\"Positivo Verdadeiro: \" + str((truePos/(n-t))*100)+\"%\")\n",
    "print(\"Negativo Falso: \" + str((fakeNeg/(n-t))*100)+\"%\")\n",
    "print(\"Negativo Verdadeiro: \" + str((trueNeg/(n-t))*100)+\"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Acerto: \" + str(((trueNeg+truePos)/(n-t))*100)+\"%\")\n",
    "print(\"Erro: \"  + str(((fakeNeg+fakePos)/(n-t))*100)+\"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Total: \" + str(((fakeNeg+fakePos+trueNeg+truePos)/(n-t))*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.[OK]<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.[OK]<br />\n",
    "Explique como são tratadas as mensagens com duplanegação e sarcasmo.[OK]<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?[OK]<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.[ ]\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.[ ]\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).[OK]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "O nosso Classificador Naive-Bayes analisa propabilidade de um tweet ser relevante baseado nas palavras usadas nele. Para conhecer a probalidade de relevancia de uma palavra fornecemos um primeiro grupo de tweet para treina-lo.\n",
    "Tivemos um percentual de analises corretas bem alta, porem o numero de Positivos Verdadeiros foi bastante baixo, isso pode estar ligada a poucos exemplos de Positivos em seu treino (Qualidade do Dataframe de Treino) e já que não existe uma analise completa do tweet, nosso clasificador não reconhece contexto, sarcasmo, duplas negações ou qualquer outro aspecto da frase que não seja a probalidade de suas palavras tornararem o proprio tweet relevante, isso faz com que o classificador cometa diverssos erros, já que podemos usar certas palavras em ambos os casos (Tweets relevantes e Irrelevantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Em nossas proximas iterações, para podermos aprimorar nosso Classificador e sem alterar o tipo dele (Naive-Bayes), vamos por exemplo:\n",
    "    \n",
    "* Ao se deparar com o Tweet: \"Eu gosto de beber Redbull sextas a noite\", alem de analisar e registrar cada uma das palavras nossa classificador passará a analisar tambem fragmentos do tweet, como: \"Eu gosto de\", \"beber Redbull\" e \"sextas a noite\"<br /> \n",
    "* Passar a não registrar artigos de ligação, como: \"de\", \"do\", \"dos\", \"da\", \"das\", etc.<br />\n",
    "* Fornecer ao Classificador um grupo de regras para transformar abreviações em sua versão completa e corrigir erros de escrita, para que dessa forma não registre duas palavras iguais como duas diferentes, tal como: \"vc\" e \"você\", aumentando a qualidade do treinamento e de analise do Classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
